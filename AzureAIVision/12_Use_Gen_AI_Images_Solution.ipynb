{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Generative AI in Images Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install requests\n",
    "%pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "azure_computer_vision_endpoint = os.environ[\"AZURE_COMPUTER_VISION_ENDPOINT\"]\n",
    "azure_computer_vision_key = os.environ[\"AZURE_COMPUTER_VISION_KEY\"]\n",
    "\n",
    "azure_ai_services_endpoint = os.environ[\"AZURE_AI_SERVICES_ENDPOINT\"]\n",
    "azure_ai_services_key = os.environ[\"AZURE_AI_SERVICES_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "# AzureOpenAI is the standard client for interacting with Azure's OpenAI Service, but it does not support asynchronous operations\n",
    "# AsyncAzureOpenAI is designed to support asynchronous operations, allowing your code to perform other tasks while waiting for the API response\n",
    "dalle_client = AsyncAzureOpenAI(\n",
    "    api_key=azure_openai_key, \n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")\n",
    "deployment_name = \"dall-e-3\"\n",
    "\n",
    "# Create the Vision client\n",
    "vision_client = AsyncAzureOpenAI(\n",
    "    api_key=azure_openai_key, \n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")\n",
    "vision_deployment_name = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate, Show and Save the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "user_prompt = \"\"\"Create an image of a playful bulldog puppy sitting in a grassy park, with a colorful ball next to it. \n",
    "The bulldog should have a happy expression and its tongue sticking out.\"\"\"\n",
    "\n",
    "# generate an image using the DALL-E 3 model\n",
    "result = await dalle_client.images.generate(\n",
    "    model=deployment_name, # the name of your DALL-E 3 deployment\n",
    "    prompt=user_prompt,\n",
    "    size=\"1024x1024\", \n",
    "    style=\"natural\",\n",
    "    quality=\"standard\",\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Retrieve the image URL from the response (assuming response structure)\n",
    "image_url = result.data[0].url\n",
    "\n",
    "# Open the image from the URL\n",
    "im = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# Save the image to a file\n",
    "im.save(\"images/dog1.jpg\")\n",
    "\n",
    "# Display the image with matplotlib\n",
    "plt.imshow(im)\n",
    "plt.axis(\"off\")  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "user_prompt = \"\"\"Generate an image of a regal-looking adult bulldog wearing a small crown, sitting on a red velvet cushion. \n",
    "The background should be a luxurious room with elegant decor.\"\"\"\n",
    "\n",
    "# generate an image using the DALL-E 3 model\n",
    "result = await dalle_client.images.generate(\n",
    "    model=deployment_name, # the name of your DALL-E 3 deployment\n",
    "    prompt=user_prompt,\n",
    "    size=\"1024x1024\", \n",
    "    style=\"natural\",\n",
    "    quality=\"standard\",\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Retrieve the image URL from the response (assuming response structure)\n",
    "image_url = result.data[0].url\n",
    "\n",
    "# Open the image from the URL\n",
    "im = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# Save the image to a file\n",
    "im.save(\"images/dog2.jpg\")\n",
    "\n",
    "# Display the image with matplotlib\n",
    "plt.imshow(im)\n",
    "plt.axis(\"off\")  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "user_prompt = \"\"\"Create an image of a poodle dressed in a cute outfit, walking down a city street with a leash in its mouth.\"\"\"\n",
    "\n",
    "# generate an image using the DALL-E 3 model\n",
    "result = await dalle_client.images.generate(\n",
    "    model=deployment_name, # the name of your DALL-E 3 deployment\n",
    "    prompt=user_prompt,\n",
    "    size=\"1024x1024\", \n",
    "    style=\"natural\",\n",
    "    quality=\"standard\",\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Retrieve the image URL from the response (assuming response structure)\n",
    "image_url = result.data[0].url\n",
    "\n",
    "# Open the image from the URL\n",
    "im = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# Save the image to a file\n",
    "im.save(\"images/dog3.jpg\")\n",
    "\n",
    "# Display the image with matplotlib\n",
    "plt.imshow(im)\n",
    "plt.axis(\"off\")  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Image and Analyze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a Path object for the image file\n",
    "image_path = Path(\"images/dog1.jpg\")\n",
    "\n",
    "# Using a context manager to open the file with Path.open()\n",
    "with image_path.open(\"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Prepare the image content in the required format for the Azure OpenAI service\n",
    "content_images = [\n",
    "    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "    for base64_image in [base64_image]\n",
    "]\n",
    "\n",
    "# Define the user prompt for the image description\n",
    "user_prompt = \"Describe this image in detail.\"\n",
    "\n",
    "# Send a request to the Azure OpenAI service to analyze the image and generate a description\n",
    "response = await vision_client.chat.completions.create(\n",
    "    model=vision_deployment_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "                *content_images,  # Include the image content in the request\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000,  # Set the maximum number of tokens for the response\n",
    ")\n",
    "\n",
    "# Print the generated description of the image\n",
    "print(\"Response: \" + response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Image API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Function to vectorize an image\n",
    "def vectorize_image(image_source, is_url=True):\n",
    "    # API URL\n",
    "    url = f\"{azure_computer_vision_endpoint}/computervision/retrieval:vectorizeImage?api-version=2024-02-01&model-version=2023-04-15\"\n",
    "\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": azure_computer_vision_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if is_url:\n",
    "            # Set headers for URL\n",
    "            headers[\"Content-Type\"] = \"application/json\"\n",
    "            data = {\n",
    "                \"url\": image_source\n",
    "            }\n",
    "            # Make the request\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "        else:\n",
    "            # Read the image file\n",
    "            with open(image_source, \"rb\") as image_file:\n",
    "                image_data = image_file.read()\n",
    "\n",
    "            # Set headers for image file\n",
    "            headers[\"Content-Type\"] = \"application/octet-stream\"\n",
    "            # Make the request\n",
    "            response = requests.post(url, headers=headers, data=image_data)\n",
    "\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Return the response\n",
    "        return response.json()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog1_result = vectorize_image(\"images/dog1.jpg\", False)\n",
    "print(\"Dog 1: \", dog1_result[\"vector\"])\n",
    "\n",
    "dog2_result = vectorize_image(\"images/dog2.jpg\", False)\n",
    "print(\"Dog 2: \", dog1_result[\"vector\"])\n",
    "\n",
    "dog3_result = vectorize_image(\"images/dog3.jpg\", False)\n",
    "print(\"Dog 3: \", dog1_result[\"vector\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Image Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dog 1 vs Dog 2\")\n",
    "print(cosine_similarity(dog1_result[\"vector\"], dog2_result[\"vector\"]))\n",
    "\n",
    "print(\"Dog 1 vs Dog 3\")\n",
    "print(cosine_similarity(dog1_result[\"vector\"], dog3_result[\"vector\"]))\n",
    "\n",
    "print(\"Dog 2 vs Dog 3\")\n",
    "print(cosine_similarity(dog2_result[\"vector\"], dog3_result[\"vector\"]))\n",
    "\n",
    "# Dog 1 and Dog 2 are more similar than Dog 1 and Dog 3 or Dog 2 and Dog 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Text API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def vectorize_text(text):\n",
    "    \n",
    "    # API URL\n",
    "    url = f\"{azure_computer_vision_endpoint}/computervision/retrieval:vectorizeText?api-version=2024-02-01&model-version=2023-04-15\"\n",
    "\n",
    "    # Set headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Ocp-Apim-Subscription-Key\": azure_computer_vision_key\n",
    "    }\n",
    "\n",
    "    # Set the data payload\n",
    "    data = {\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Make the request\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Return the JSON response\n",
    "        return response.json()\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"bulldog\"\n",
    "text_bulldog_vector = vectorize_text(user_input)\n",
    "print(\"Bulldog: \", text_bulldog_vector[\"vector\"])\n",
    "\n",
    "user_input = \"poodle\"\n",
    "text_poodle_vector = vectorize_text(user_input)\n",
    "print(\"Poodle: \", text_poodle_vector[\"vector\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Similarity between Text Input and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bulldog Text vs Images\")\n",
    "print(cosine_similarity(text_bulldog_vector[\"vector\"], dog1_result[\"vector\"]))\n",
    "print(cosine_similarity(text_bulldog_vector[\"vector\"], dog2_result[\"vector\"]))\n",
    "print(cosine_similarity(text_bulldog_vector[\"vector\"], dog3_result[\"vector\"]))\n",
    "\n",
    "print(\"Poodle Text vs Images\")\n",
    "print(cosine_similarity(text_poodle_vector[\"vector\"], dog1_result[\"vector\"]))\n",
    "print(cosine_similarity(text_poodle_vector[\"vector\"], dog2_result[\"vector\"]))\n",
    "print(cosine_similarity(text_poodle_vector[\"vector\"], dog3_result[\"vector\"]))\n",
    "\n",
    "# Bulldog text is more similar to the first and second image vs the third image\n",
    "# Poodle text is more similar to the third image vs the first and second image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
