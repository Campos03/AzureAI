{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG solution using Azure AI Search\n",
    "\n",
    "Steps in this notebook:\n",
    "1. Create an index \n",
    "2. Create a data source\n",
    "3. Create a skillset\n",
    "4. Create an indexer \n",
    "5. Send a query to the search engine to check results\n",
    "6. Send query results to a language model to generate response\n",
    "\n",
    "Note: Steps 1-4: Done during initial setup of Search Index only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install azure-core\n",
    "%pip install azure-search-documents\n",
    "%pip install azure-storage-blob\n",
    "%pip install azure-identity\n",
    "%pip install openai\n",
    "%pip install aiohttp\n",
    "%pip install ipywidgets\n",
    "%pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure configurations\n",
    "\n",
    "You always need to run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_embeddings_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT\")\n",
    "azure_openai_api_version = \"2024-10-01-preview\"\n",
    "azure_openai_embedding_size = 1536\n",
    "azure_search_service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "azure_search_service_admin_key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "azure_search_service_index_name = \"ai-search-index-001\"\n",
    "azure_storage_connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "azure_ai_services_key = os.getenv(\"AZURE_AI_MULTISERVICE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai-search-index-001 created\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    ScoringProfile,\n",
    "    TagScoringFunction,\n",
    "    TagScoringParameters\n",
    ")\n",
    "\n",
    "# Get credential from Azure AI Search Admin key\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "\n",
    "# Search index name  \n",
    "index_name = azure_search_service_index_name\n",
    "\n",
    "# Create a Search Index Client\n",
    "index_client = SearchIndexClient(endpoint=azure_search_service_endpoint, credential=credential)\n",
    "\n",
    "# Define the fields collection\n",
    "fields = [\n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String),  \n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"locations\", type=SearchFieldDataType.Collection(SearchFieldDataType.String), filterable=True),\n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),  \n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),  \n",
    "    SearchField(name=\"text_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=azure_openai_embedding_size, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]  \n",
    "  \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(name=\"myHnsw\"),\n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "        )\n",
    "    ],  \n",
    "    vectorizers=[   # a vectorizer is software that performs vectorization\n",
    "        AzureOpenAIVectorizer(  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            parameters=AzureOpenAIVectorizerParameters(  \n",
    "                resource_url=azure_openai_endpoint,  \n",
    "                deployment_name=azure_openai_embeddings_deployment,\n",
    "                model_name=azure_openai_embeddings_deployment\n",
    "            ),\n",
    "        ),  \n",
    "    ], \n",
    ")  \n",
    "\n",
    "# New semantic configuration\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        keywords_fields=[SemanticField(field_name=\"locations\")],\n",
    "        content_fields=[SemanticField(field_name=\"chunk\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# New scoring profile\n",
    "scoring_profiles = [  \n",
    "    ScoringProfile(  \n",
    "        name=\"my-scoring-profile\",\n",
    "        functions=[\n",
    "            TagScoringFunction(  \n",
    "                field_name=\"locations\",  \n",
    "                boost=5.0,  \n",
    "                parameters=TagScoringParameters(  \n",
    "                    tags_parameter=\"tags\",  \n",
    "                ),  \n",
    "            ) \n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create the search index\n",
    "index = SearchIndex(name=index_name, \n",
    "                    fields=fields, \n",
    "                    vector_search=vector_search,\n",
    "                    semantic_search=semantic_search,\n",
    "                    scoring_profiles=scoring_profiles)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Azure Storage access\n",
    "Verify access to storage and print out the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blobs in the container:\n",
      "page-11.pdf\n",
      "page-13.pdf\n",
      "page-15.pdf\n",
      "page-17.pdf\n",
      "page-19.pdf\n",
      "page-21.pdf\n",
      "page-23.pdf\n",
      "page-25.pdf\n",
      "page-27.pdf\n",
      "page-31.pdf\n",
      "page-33.pdf\n",
      "page-35.pdf\n",
      "page-39.pdf\n",
      "page-41.pdf\n",
      "page-43.pdf\n",
      "page-45.pdf\n",
      "page-49.pdf\n",
      "page-51.pdf\n",
      "page-55.pdf\n",
      "page-57.pdf\n",
      "page-59.pdf\n",
      "page-61.pdf\n",
      "page-63.pdf\n",
      "page-65.pdf\n",
      "page-67.pdf\n",
      "page-69.pdf\n",
      "page-7.pdf\n",
      "page-71.pdf\n",
      "page-8.pdf\n",
      "page-9.pdf\n",
      "Access to the blob storage was granted.\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Initialize the BlobServiceClient with the connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(azure_storage_connection_string)\n",
    "\n",
    "# Get the container client\n",
    "container_client = blob_service_client.get_container_client(\"nasabooks\")\n",
    "\n",
    "# List blobs in the container\n",
    "try:\n",
    "    blobs_list = container_client.list_blobs()\n",
    "    print(\"Blobs in the container:\")\n",
    "    for blob in blobs_list:\n",
    "        print(blob.name)\n",
    "    print(\"Access to the blob storage was granted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to access the blob storage: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'ai-search-ds' created or updated\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection\n",
    ")\n",
    "\n",
    "# Create a data source \n",
    "indexer_client = SearchIndexerClient(endpoint=azure_search_service_endpoint, credential=credential)\n",
    "container = SearchIndexerDataContainer(name=\"nasabooks\")\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=\"ai-search-ds\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=azure_storage_connection_string,\n",
    "    container=container\n",
    ")\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai-search-ss created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    EntityRecognitionSkill,\n",
    "    SearchIndexerIndexProjection,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    SearchIndexerSkillset,\n",
    "    CognitiveServicesAccountKey\n",
    ")\n",
    "\n",
    "# Create a skillset  \n",
    "skillset_name = \"ai-search-ss\"\n",
    "\n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=2000,  \n",
    "    page_overlap_length=500,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/pages/*\",  \n",
    "    resource_url=azure_openai_endpoint,  \n",
    "    deployment_name=azure_openai_embeddings_deployment,  \n",
    "    model_name=azure_openai_embeddings_deployment,\n",
    "    dimensions=azure_openai_embedding_size,\n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"text_vector\")  \n",
    "    ],  \n",
    ")\n",
    "\n",
    "entity_skill = EntityRecognitionSkill(\n",
    "    description=\"Skill to recognize entities in text\",\n",
    "    context=\"/document/pages/*\",\n",
    "    categories=[\"Location\"],\n",
    "    default_language_code=\"en\",\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        OutputFieldMappingEntry(name=\"locations\", target_name=\"locations\")\n",
    "    ]\n",
    ")\n",
    "  \n",
    "index_projections = SearchIndexerIndexProjection(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=azure_search_service_index_name,  \n",
    "            parent_key_field_name=\"parent_id\",  \n",
    "            source_context=\"/document/pages/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),  \n",
    "                InputFieldMappingEntry(name=\"text_vector\", source=\"/document/pages/*/text_vector\"),\n",
    "                InputFieldMappingEntry(name=\"locations\", source=\"/document/pages/*/locations\"),  \n",
    "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),  \n",
    "            ],  \n",
    "        ),  \n",
    "    ],  \n",
    "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "    ),  \n",
    ") \n",
    "\n",
    "cognitive_services_account = CognitiveServicesAccountKey(key=azure_ai_services_key)\n",
    "\n",
    "skills = [split_skill, embedding_skill, entity_skill]\n",
    "\n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents, generate embeddings, and extract location entities\",  \n",
    "    skills=skills,  \n",
    "    index_projection=index_projections,\n",
    "    cognitive_services_account=cognitive_services_account\n",
    ")\n",
    "  \n",
    "client = SearchIndexerClient(endpoint=azure_search_service_endpoint, credential=credential)  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f\"{skillset.name} created\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ai-search-idxr is created and running. Give the indexer a few minutes before running a query.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    IndexingSchedule\n",
    ")\n",
    "\n",
    "# Create an indexer  \n",
    "indexer_name = \"ai-search-idxr\" \n",
    "\n",
    "# Schedule to run every 24 hours\n",
    "schedule = IndexingSchedule(interval=\"P1D\")\n",
    "\n",
    "indexer_parameters = None\n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents, generate embeddings, and extract entities\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name,\n",
    "    parameters=indexer_parameters,\n",
    "    schedule=schedule\n",
    ")  \n",
    "\n",
    "# Create and run the indexer  \n",
    "indexer_client = SearchIndexerClient(endpoint=azure_search_service_endpoint, credential=credential)  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "\n",
    "print(f' {indexer_name} is created and running. Give the indexer a few minutes before running a query.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a query to the search engine to check results\n",
    "\n",
    "Executed every time a user makes a query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.03279569745063782 \n",
      "\n",
      "Title: page-21.pdf \n",
      "\n",
      "Chunk: A\n",
      "T\n",
      "\n",
      "M\n",
      "O\n",
      "\n",
      "S\n",
      "P\n",
      "\n",
      "H\n",
      "E\n",
      "\n",
      "R\n",
      "E\n",
      "\n",
      "E\n",
      "A\n",
      "\n",
      "R\n",
      "T\n",
      "\n",
      "H\n",
      "\n",
      "14\n",
      "\n",
      "Bering Streets\n",
      "Arctic Ocean\n",
      "\n",
      "Winds from the northeast pushed sea ice southward and formed cloud streets—parallel rows of clouds—over the Bering Strait in \n",
      "\n",
      "January 2010. The easternmost reaches of Russia, blanketed in snow and ice, appear in the upper left. To the east, sea ice spans \n",
      "\n",
      "the Bering Strait. Along the southern edge of the ice, wavy tendrils of newly formed, thin sea ice predominate.\n",
      "\n",
      "The cloud streets run in the direction of the northerly wind that helps form them. When wind blows out from a cold surface like sea \n",
      "\n",
      "ice over the warmer, moister air near the open ocean, cylinders of spinning air may develop. Clouds form along the upward cycle in \n",
      "\n",
      "the cylinders, where air is rising, and skies remain clear along the downward cycle, where air is falling. The cloud streets run toward \n",
      "\n",
      "the southwest in this image from the Terra satellite. \n",
      "\n",
      "Locations: ['Arctic Ocean', 'streets', 'Bering Strait', 'reaches', 'Russia', 'surface', 'sea', 'ocean']\n",
      "Score: 0.0320020467042923 \n",
      "\n",
      "Title: page-63.pdf \n",
      "\n",
      "Chunk: W\n",
      "a\n",
      "\n",
      "t\n",
      "e\n",
      "\n",
      "r\n",
      "\n",
      "56\n",
      "\n",
      "A Lava Lamp Look at the Atlantic\n",
      "Atlantic Ocean\n",
      "\n",
      "Stretching from tropical Florida to the doorstep of Europe, the Gulf Stream carries a lot of heat, salt, and history. This river of water is \n",
      "\n",
      "an important part of the global ocean conveyor belt, moving water and heat from the Equator toward the far North Atlantic. It is one \n",
      "\n",
      "of the strongest currents on Earth and one of the most studied. Its discovery is often attributed to Benjamin Franklin, though sailors \n",
      "\n",
      "likely knew about the current long before they had a name for it.\n",
      "\n",
      "This image shows a small portion of the Gulf Stream off of South Carolina as it appeared in infrared data collected by the Landsat 8 \n",
      "\n",
      "satellite in April 2013. Colors represent the energy—heat—being emitted by the water, with cooler temperatures in purple and the \n",
      "\n",
      "warmest water being nearly white. Note how the Gulf Stream is not a uniform band but instead has finer streams and pockets of \n",
      "\n",
      "warmer and colder water. \n",
      "\n",
      "Locations: ['Atlantic', 'Atlantic Ocean', 'Florida', 'Europe', 'Gulf Stream', 'river', 'Equator', 'North Atlantic', 'Earth', 'South Carolina']\n",
      "Score: 0.0314980149269104 \n",
      "\n",
      "Title: page-39.pdf \n",
      "\n",
      "Chunk: A\n",
      "T\n",
      "\n",
      "M\n",
      "O\n",
      "\n",
      "S\n",
      "P\n",
      "\n",
      "H\n",
      "E\n",
      "\n",
      "R\n",
      "E\n",
      "\n",
      "E\n",
      "A\n",
      "\n",
      "R\n",
      "T\n",
      "\n",
      "H\n",
      "\n",
      "32\n",
      "\n",
      "Framing an Iceberg\n",
      "South Atlantic Ocean\n",
      "\n",
      "In June 2016, the Suomi NPP satellite captured this image of various cloud formations in the South Atlantic Ocean. Note how low \n",
      "\n",
      "stratus clouds framed a hole over iceberg A-56 as it drifted across the sea. \n",
      "\n",
      "The exact reason for the hole in the clouds is somewhat of a mystery. It could have formed by chance, although imagery from the \n",
      "\n",
      "days before and after this date suggest something else was at work. It could be that the relatively unobstructed path of the clouds \n",
      "\n",
      "over the ocean surface was interrupted by thermal instability created by the iceberg. In other words, if an obstacle is big enough,  \n",
      "\n",
      "it can divert the low-level atmospheric flow of air around it, a phenomenon often caused by islands. \n",
      "\n",
      "Locations: ['Iceberg', 'South Atlantic Ocean', 'Suomi', 'iceberg A-56', 'sea', 'clouds', 'ocean', 'iceberg', 'islands']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Get credential from Azure AI Search Admin key\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, \n",
    "                             credential=credential, \n",
    "                             index_name=azure_search_service_index_name)\n",
    "\n",
    "# User Query\n",
    "query = \"Whats in the Arctic ocean and what is special about it?\"  \n",
    "\n",
    "# Convert query into vector form\n",
    "vector_query = VectorizableTextQuery(text=query, \n",
    "                                     k_nearest_neighbors=50, \n",
    "                                     fields=\"text_vector\",\n",
    "                                     weight=1)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  #This performs a full-text search using the query\n",
    "    vector_queries= [vector_query], #This adds a vector search component\n",
    "    select=[\"title\",\"chunk\",\"locations\"], #Specify fields to be return in the result\n",
    "    top=3\n",
    ") \n",
    "\n",
    "for result in results:  \n",
    "    print(f\"Score: {result['@search.score']} \\n\")\n",
    "    print(f\"Title: {result['title']} \\n\")\n",
    "    print(f\"Chunk: {result['chunk']} \\n\")\n",
    "    print(f\"Locations: {result['locations']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Semantic Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.029012346640229225 \n",
      "\n",
      "Title: page-63.pdf \n",
      "\n",
      "Chunk: W\n",
      "a\n",
      "\n",
      "t\n",
      "e\n",
      "\n",
      "r\n",
      "\n",
      "56\n",
      "\n",
      "A Lava Lamp Look at the Atlantic\n",
      "Atlantic Ocean\n",
      "\n",
      "Stretching from tropical Florida to the doorstep of Europe, the Gulf Stream carries a lot of heat, salt, and history. This river of water is \n",
      "\n",
      "an important part of the global ocean conveyor belt, moving water and heat from the Equator toward the far North Atlantic. It is one \n",
      "\n",
      "of the strongest currents on Earth and one of the most studied. Its discovery is often attributed to Benjamin Franklin, though sailors \n",
      "\n",
      "likely knew about the current long before they had a name for it.\n",
      "\n",
      "This image shows a small portion of the Gulf Stream off of South Carolina as it appeared in infrared data collected by the Landsat 8 \n",
      "\n",
      "satellite in April 2013. Colors represent the energy—heat—being emitted by the water, with cooler temperatures in purple and the \n",
      "\n",
      "warmest water being nearly white. Note how the Gulf Stream is not a uniform band but instead has finer streams and pockets of \n",
      "\n",
      "warmer and colder water. \n",
      "\n",
      "Locations: ['Atlantic', 'Atlantic Ocean', 'Florida', 'Europe', 'Gulf Stream', 'river', 'Equator', 'North Atlantic', 'Earth', 'South Carolina']\n",
      "Score: 0.027912385761737823 \n",
      "\n",
      "Title: page-31.pdf \n",
      "\n",
      "Chunk: A\n",
      "T\n",
      "\n",
      "M\n",
      "O\n",
      "\n",
      "S\n",
      "P\n",
      "\n",
      "H\n",
      "E\n",
      "\n",
      "R\n",
      "E\n",
      "\n",
      "E\n",
      "A\n",
      "\n",
      "R\n",
      "T\n",
      "\n",
      "H\n",
      "\n",
      "24\n",
      "\n",
      "Making Tracks\n",
      "Pacific Ocean\n",
      "\n",
      "Ships steaming across the Pacific Ocean left this cluster of bright cloud trails lingering in the atmosphere in February 2012. The \n",
      "\n",
      "narrow clouds, known as ship tracks, form when water vapor condenses around tiny particles of pollution from ship exhaust. The \n",
      "\n",
      "crisscrossing clouds off the coast of California stretched for many hundreds of kilometers from end to end. The narrow ends of the \n",
      "\n",
      "clouds are youngest, while the broader, wavier ends are older.\n",
      "\n",
      "Some of the pollution particles generated by ships (especially sulfates) are soluble in water and can serve as the seeds around which \n",
      "\n",
      "cloud droplets form. Clouds infused with ship exhaust have more and smaller droplets than unpolluted clouds. As a result, light \n",
      "\n",
      "hitting the ship tracks scatters in many directions, often making them appear brighter than other types of marine clouds, which are \n",
      "\n",
      "usually seeded by larger, naturally occurring particles like sea salt. \n",
      "\n",
      "Locations: ['Pacific Ocean', 'coast', 'California', 'clouds']\n",
      "Score: 0.026860956102609634 \n",
      "\n",
      "Title: page-17.pdf \n",
      "\n",
      "Chunk: A\n",
      "T\n",
      "\n",
      "M\n",
      "O\n",
      "\n",
      "S\n",
      "P\n",
      "\n",
      "H\n",
      "E\n",
      "\n",
      "R\n",
      "E\n",
      "\n",
      "E\n",
      "A\n",
      "\n",
      "R\n",
      "T\n",
      "\n",
      "H\n",
      "\n",
      "10\n",
      "\n",
      "A Glorious View\n",
      "Pacific Ocean\n",
      "\n",
      "A layer of stratocumulus clouds over the Pacific Ocean serves as the backdrop for this rainbow-like phenomenon known as a glory. \n",
      "\n",
      "Glories form when water droplets within clouds scatter sunlight back toward a source of illumination (in this case, the Sun). \n",
      "\n",
      "Although glories may look similar to rainbows, the way light is scattered to produce them is different. Rainbows are formed by \n",
      "\n",
      "refraction and reflection; glories are formed by backward diffraction. From the ground or from an airplane, glories appear as circular \n",
      "\n",
      "rings of color. In this image, however, the glory is stretched vertically because of how the imager scans the surface in swaths.\n",
      "\n",
      "Note, too, the swirling von Kármán vortices visible to the right of the glory. The alternating rows of vortices form as air masses run \n",
      "\n",
      "into an obstacle—the island of Guadalupe—and form a wake behind it. \n",
      "\n",
      "Locations: ['Pacific Ocean', 'Sun', 'ground', 'island', 'Guadalupe']\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Get credential from Azure AI Search Admin key\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, \n",
    "                             credential=credential, \n",
    "                             index_name=azure_search_service_index_name)\n",
    "\n",
    "# User Query\n",
    "query = \"What can I see in the beach in the United States?\"  \n",
    "\n",
    "# Convert query into vector form\n",
    "vector_query = VectorizableTextQuery(text=query, \n",
    "                                     k_nearest_neighbors=50, \n",
    "                                     fields=\"text_vector\",\n",
    "                                     weight=1)\n",
    "\n",
    "results = search_client.search(\n",
    "    query_type=\"semantic\", \n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    scoring_profile=\"my-scoring-profile\",\n",
    "    scoring_parameters=[\"tags-beach, 'United States'\"], \n",
    "    search_text=query,\n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\",\"chunk\",\"locations\"],\n",
    "    top=3,\n",
    ")\n",
    "\n",
    "# Sort the results by score in descending order\n",
    "sorted_results = sorted(results, key=lambda x: x['@search.score'], reverse=True)\n",
    "\n",
    "for result in sorted_results:  \n",
    "    print(f\"Score: {result['@search.score']} \\n\")\n",
    "    print(f\"Title: {result['title']} \\n\")\n",
    "    print(f\"Chunk: {result['chunk']} \\n\")\n",
    "    print(f\"Locations: {result['locations']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send query results to a language model to generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pacific Ocean comprises various natural phenomena and features, including:\n",
      "\n",
      "- **Cloud formations**: Ship tracks and stratocumulus clouds are common, with ship tracks formed by water vapor condensing around pollution particles from ships (source: page-31.pdf). Stratocumulus clouds creating a \"glory\" effect are also observed (source: page-17.pdf).\n",
      "- **Volcanic Islands**: The Islands of the Four Mountains include Carlisle, Cleveland, Herbert, and Tana, which are peaks of volcanoes rising from the seafloor in the Aleutian Island chain (source: page-35.pdf).\n",
      "- **Marine Clouds**: Along the coast of Peru, valleys are often filled with marine stratocumulus clouds that can move inland due to prevailing winds (source: page-15.pdf).\n",
      "- **Hurricanes**: Hurricanes Madeline and Lester stirred the central Pacific as category 3 and 4 storms in August 2016, although they did not make landfall on Hawaii directly (source: page-27.pdf).\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Get credential from Azure AI Search Admin key\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, \n",
    "                             credential=credential, \n",
    "                             index_name=azure_search_service_index_name)\n",
    "\n",
    "# Azure OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "    # to get version: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key)\n",
    "\n",
    "# Provide instructions to the model\n",
    "SYSTEM_PROMPT=\"\"\"\n",
    "You are an AI assistant that helps users learn from the information found in the source material.\n",
    "Answer the query using only the sources provided below.\n",
    "Use bullets if the answer has multiple points.\n",
    "If the answer is longer than 3 sentences, provide a summary.\n",
    "Answer ONLY with the facts listed in the list of sources below. Cite your source when you answer the question\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "\n",
    "# User Query\n",
    "query = \"Whats in the Pacific Ocean?\"  \n",
    "\n",
    "# Convert query into vector form\n",
    "vector_query = VectorizableTextQuery(text=query, \n",
    "                                     k_nearest_neighbors=50, \n",
    "                                     fields=\"text_vector\",\n",
    "                                     weight=1)\n",
    "\n",
    "results = search_client.search(\n",
    "    query_type=\"semantic\", \n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    search_text=query,\n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\",\"chunk\",\"locations\"],\n",
    "    top=5,\n",
    ")\n",
    "\n",
    "# Use a unique separator to make the sources distinct. \n",
    "# We chose repeated equal signs (=) followed by a newline because it's unlikely the source documents contain this sequence.\n",
    "sources_formatted = \"=================\\n\".join([f'TITLE: {document[\"title\"]}, CONTENT: {document[\"chunk\"]}, LOCATIONS: {document[\"locations\"]}' for document in results])\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": SYSTEM_PROMPT.format(query=query, sources=sources_formatted)\n",
    "        }\n",
    "    ],\n",
    "    model=azure_openai_deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
