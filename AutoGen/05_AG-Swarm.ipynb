{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGen Swarm\n",
    "Swarm implements a team in which agents can hand off task to other agents based on their capabilities. It is a multi-agent design pattern first introduced by OpenAI in an experimental project. The key idea is to let agent delegate tasks to other agents using a special tool call, while all agents share the same message context. This enables agents to make local decisions about task planning, rather than relying on a central orchestrator such as in SelectorGroupChat.\n",
    "\n",
    "The speaker agent is selected based on the most recent HandoffMessage message in the context. This naturally requires each agent in the team to be able to generate HandoffMessage to signal which other agents that it hands off to.\n",
    "\n",
    "In general, this is how it works:\n",
    "\n",
    "1. Each agent has the ability to generate HandoffMessage to signal which other agents it can hand off to. For AssistantAgent, this means setting the handoffs argument.\n",
    "\n",
    "2. When the team starts on a task, the first speaker agents operate on the task and make locallized decision about whether to hand off and to whom.\n",
    "\n",
    "3. When an agent generates a HandoffMessage, the receiving agent takes over the task with the same message context.\n",
    "\n",
    "4. The process continues until a termination condition is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Create the token provider\n",
    "#token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=azure_openai_deployment,\n",
    "    model=azure_openai_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    # azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.\n",
    "    api_key=azure_openai_key, # For key-based authentication.\n",
    ")\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Create the Dalle client\n",
    "dalle_client = AzureOpenAI(\n",
    "    api_key=azure_openai_key, \n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")\n",
    "dalle_deployment_name = \"dall-e-3\"\n",
    "\n",
    "# Create the Vision client\n",
    "vision_client = AzureOpenAI(\n",
    "    api_key=azure_openai_key, \n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")\n",
    "vision_deployment_name = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import Swarm\n",
    "from autogen_agentchat.ui import Console\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import requests\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Define a tool\n",
    "# This function calls the Dalle-3 image generator given the prompt and displays the generated image.\n",
    "def generate_image(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Call the Azure OpenAI Dall-e 3 model to generate an image from a text prompt.\n",
    "    Executes the call to the Azure OpenAI Dall-e 3 image creator, saves the file into the local directory, and displays the image.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Dalle Assistant Message: Creating the image ...\")\n",
    "\n",
    "    response = dalle_client.images.generate(\n",
    "        model=dalle_deployment_name, prompt=prompt, size=\"1024x1024\", quality=\"standard\", n=1\n",
    "    )\n",
    "\n",
    "    # Retrieve the image URL from the response (assuming response structure)\n",
    "    image_url = response.data[0].url\n",
    "\n",
    "    # Open the image from the URL and save it to a temporary file.\n",
    "    im = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "    # Define the filename and path where the image should be saved.\n",
    "    filename = \"temp.jpg\"\n",
    "    local_path = Path(filename)\n",
    "\n",
    "    # Save the image.\n",
    "    im.save(local_path)\n",
    "\n",
    "    # Get the absolute path of the saved image.\n",
    "    full_path = str(local_path.absolute())\n",
    "\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    # Convert the image from BGR to RGB for displaying with matplotlib,\n",
    "    # because OpenCV uses BGR by default and matplotlib expects RGB.\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image with matplotlib.\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis(\"off\")  # Turn off axis labels.\n",
    "    plt.show()\n",
    "\n",
    "    # Return the full path of the saved image.\n",
    "    print(\"Dalle Assistant Message: \" + full_path)\n",
    "    return \"Image generated successfully and store in the local file system. You can now use this image to analyze it with the vision_assistant\"\n",
    "\n",
    "# Define an agent\n",
    "dalle_assistant = AssistantAgent(\n",
    "    name=\"dalle_assistant\",\n",
    "    description=\"This agent calls the Azure OpenAI Dall-e 3 model to generate an image from a text prompt coming from the user or a vision assistant.\",\n",
    "    handoffs=[\"planning_agent\"],\n",
    "    model_client=az_model_client,\n",
    "    tools=[generate_image],\n",
    "    system_message=\"\"\"\n",
    "    As a premier AI specializing in image generation, you possess the expertise to craft precise visuals based on given prompts. \n",
    "    It is essential that you diligently generate the requested image, ensuring its accuracy and alignment with the user's specifications, \n",
    "    prior to delivering a response.\n",
    "\n",
    "    Always handoff back to planner when image generation is complete.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the image\n",
    "def analyze_image() -> str:\n",
    "    \"\"\"\n",
    "    Call the Azure OpenAI GPT4 Vision model to analyze and critic an image and return the result.The resulting output should be a new prompt for dall-e that enhances the image based on the criticism and analysis\n",
    "    \"\"\"\n",
    "    print(\"Vision Assistant Message: \" + \"Analyzing the image...\")\n",
    "\n",
    "    import base64\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create a Path object for the image file\n",
    "    image_path = Path(\"temp.jpg\")\n",
    "\n",
    "    # Using a context manager to open the file with Path.open()\n",
    "    with image_path.open(\"rb\") as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    content_images = [\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "        for base64_image in [base64_image]\n",
    "    ]\n",
    "    response = vision_client.chat.completions.create(\n",
    "        model=vision_deployment_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Analyze and critic this image and generate a new enhanced prompt for Dall-e with the criticism and analysis.\",\n",
    "                    },\n",
    "                    *content_images,\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    print(\"Vision Assistant Message: \" + response.choices[0].message.content)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Creating the vision assistant agent\n",
    "vision_assistant = AssistantAgent(\n",
    "    name=\"vision_assistant\",\n",
    "    description=\"This agent analyzes and critic an image and return the result. The resulting output should be a new prompt for dalle assistant that enhances the image based on the criticism and analysis\",\n",
    "    handoffs=[\"planning_agent\"],\n",
    "    model_client=az_model_client,\n",
    "    tools=[analyze_image],\n",
    "    system_message=\"\"\"\n",
    "    As a leading AI expert in image analysis, you excel at scrutinizing and offering critiques to refine and improve images. \n",
    "    Your task is to thoroughly analyze an image, ensuring that all essential assessments are completed with precision \n",
    "    before you provide feedback to the user. You have access to the local file system where the image is stored.\n",
    "\n",
    "    Always handoff back to planner when image analysis is complete.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the user proxy agent.\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the planning agent\n",
    "planning_agent = AssistantAgent(\n",
    "    \"planning_agent\",\n",
    "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
    "    handoffs=[\"user_proxy\", \"dalle_assistant\", \"vision_assistant\"],\n",
    "    model_client=az_model_client,\n",
    "    system_message=f\"\"\"\n",
    "    You are a planning agent that coordinates the work of your team members.\n",
    "    Coordinate handoff to specialized agents:\n",
    "    - dalle assistant for image generation\n",
    "    - vision assistant for image analysis and critique\n",
    "    - dalle assistant for enhancing the image based on the analysis\n",
    "    - final handoff to user proxy for final approval\n",
    "\n",
    "     You will follow this sequence:\n",
    "        Step 1: Dalle Assistant will generate an image based on the initial user prompt and display it for review.\n",
    "        Step 2: Vision Assistant will analyze the image and provide a new prompt for Dalle Assistant to generate a new image based on the new prompt.\n",
    "        Step 3: Dalle Assistant will generate an image based on the Vision Assistant prompt and display it for review.\n",
    "        Step 4: Vision Assistant will analyze the image and provide a new prompt for Dalle Assistant to generate a new image based on the new prompt.\n",
    "        Step 5: Dalle Assistant will generate an image based on the Vision Assistant prompt and display it for review.\n",
    "        Step 6: Vision Assistant will analyze the image and provide a new prompt for Dalle Assistant to generate a new image based on the new prompt.\n",
    "        Step 7: User Proxy will provide feedback on process.\n",
    "\n",
    "    Always send your plan first, then handoff to appropriate agent.\n",
    "    Always handoff to a single agent at a time.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mention_termination = TextMentionTermination(\"APPROVE\")\n",
    "max_messages_termination = MaxMessageTermination(max_messages=100)\n",
    "termination = text_mention_termination | max_messages_termination\n",
    "\n",
    "team = Swarm(\n",
    "    participants=[planning_agent, dalle_assistant, vision_assistant, user_proxy], termination_condition=termination\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Task and Run the Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Generate an image of a boat drifting in the water and analyze it and enhance the image\"\n",
    "response = await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing whole response per agent source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape code for bold text\n",
    "bold_start = \"\\033[1m\"\n",
    "bold_end = \"\\033[0m\"\n",
    "\n",
    "# ANSI escape code for red text\n",
    "red_start = \"\\033[31m\"\n",
    "red_end = \"\\033[0m\"\n",
    "\n",
    "for messages in response.messages:\n",
    "    source = messages.source\n",
    "    print(f\"{bold_start}{red_start}{source}{bold_end}{red_end}\")\n",
    "    print(messages.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
