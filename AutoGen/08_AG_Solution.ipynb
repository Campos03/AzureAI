{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selector Group Chat\n",
    "SelectorGroupChat implements a team where participants take turns broadcasting messages to all other members. A generative model (e.g., an LLM) selects the next speaker based on the shared context, enabling dynamic, context-aware collaboration.\n",
    "\n",
    "Key features include:\n",
    "- Model-based speaker selection\n",
    "- Configurable participant roles and descriptions\n",
    "- Prevention of consecutive turns by the same speaker (optional)\n",
    "- Customizable selection prompting\n",
    "- Customizable selection function to override the default model-based selection\n",
    "\n",
    "SelectorGroupChat is a group chat similar to RoundRobinGroupChat, but with a model-based next speaker selection mechanism. When the team receives a task through run() or run_stream(), the following steps are executed:\n",
    "1. The team analyzes the current conversation context, including the conversation history and participants’ name and description attributes, to determine the next speaker using a model. You can override the model by providing a custom selection function.\n",
    "2. The team prompts the selected speaker agent to provide a response, which is then broadcasted to all other participants.\n",
    "3. The termination condition is checked to determine if the conversation should end, if not, the process repeats from step 1.\n",
    "4. When the conversation ends, the team returns the TaskResult containing the conversation history from this task.\n",
    "\n",
    "Once the team finishes the task, the conversation context is kept within the team and all participants, so the next task can continue from the previous conversation context. You can reset the conversation context by calling reset()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure OpenAI Client\n",
    "Using the model client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Create the token provider\n",
    "#token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=azure_openai_deployment,\n",
    "    model=azure_openai_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    # azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.\n",
    "    api_key=azure_openai_key, # For key-based authentication.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.messages import AgentEvent, ChatMessage\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.agents.web_surfer import MultimodalWebSurfer\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# Get the current date\n",
    "current_date = date.today()\n",
    "\n",
    "planning_agent = AssistantAgent(\n",
    "    \"PlanningAgent\",\n",
    "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
    "    model_client=az_model_client,\n",
    "    system_message=f\"\"\"\n",
    "    You are a planning agent.\n",
    "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
    "    Your team members are:\n",
    "        Multimodal Web Surfer: Searches for information in the internet on the weather advisory for the current date: {current_date} on the hiking location.\n",
    "        Hiking Agent: Provides a hiking plan for the hiker based on the weather advisory.\n",
    "        Product Agent: Recommends products from its file search tool.\n",
    "        Writing Agent: Writes a document combining the hiking plan, product suggestions, and weather advisory.\n",
    "        Critique Agent: Critiques the hiking plan, product agent, and writing agent and suggests improvements.\n",
    "        \n",
    "        \n",
    "    You only plan and delegate tasks - you do not execute them yourself.\n",
    "\n",
    "    When assigning tasks, use this format:\n",
    "    1. <agent> : <task>\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "web_surfer_agent = MultimodalWebSurfer(\n",
    "    name=\"MultimodalWebSurfer\",\n",
    "    model_client=az_model_client,\n",
    "    description=\"A web surfer agent that searches for information on the internet.\",\n",
    ")\n",
    "\n",
    "hiking_agent = AssistantAgent(\n",
    "    \"HikingAgent\",\n",
    "    description=\"A Hiking agent. Writes a hiking plan based on the weather advisory.\",\n",
    "    model_client=az_model_client,\n",
    "    system_message=\"\"\"\n",
    "    You are a Hiking agent.\n",
    "    You will create a hiking plan based on the weather advisory.\n",
    "    After writing the hiking plan, you will pass it to the Critique agent for review.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "critique_agent = AssistantAgent(\n",
    "    \"CritiqueAgent\",\n",
    "    description=\"A critique agent. Critiques the Hiking plan, product suggestions, and writing agent.\",\n",
    "    model_client=az_model_client,\n",
    "    system_message=\"\"\"\n",
    "    You are a Critique agent.\n",
    "    You need to ensure that the hiking plan is well thought out and provides constructive feedback.\n",
    "    You need to ensure the multi-modal web surfer has provided the correct information on the weather\n",
    "    You need to ensure that the product agent provides products from file search tool such as the CozyNights Sleeping Bag, BaseCamp Folding Table, and many more.\n",
    "    You need to ensure that the writing agent has combined the hiking plan, the product suggestions, and weather advisory into a single document.\n",
    "    You need to ensure feedback from user is incorporated and ask agents to make necessary changes.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Define a tool to write the content to a file\n",
    "async def write_to_file(content: str) -> str:\n",
    "    # Write the content to a file\n",
    "    with open(\"output/output.txt\", \"w\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "writing_agent = AssistantAgent(\n",
    "    \"WritingAgent\",\n",
    "    description=\"A Writing agent that combines the hiking plan, product suggestions, and weather advisory into a single document.\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[write_to_file],\n",
    "    system_message=\"\"\"\n",
    "    You are a writing agent.\n",
    "    You will combine the hiking plan, the product suggestions, and weather advisory into a single document.\n",
    "    You will write the final output to a file.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Create the user proxy agent.\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.agents.openai import OpenAIAssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "cancellation_token = CancellationToken()\n",
    "\n",
    "# Create AzureOpenAI client\n",
    "client = AsyncAzureOpenAI(azure_endpoint=azure_openai_endpoint, \n",
    "                          api_version=azure_openai_api_version, \n",
    "                          api_key=azure_openai_key)\n",
    "\n",
    "#### Create a vector store for file search ####\n",
    "vector_store = await client.beta.vector_stores.create(name=\"Hiking Products\")\n",
    "# Specify the folder containing the files\n",
    "folder_path = \"../Data/products/\"\n",
    "# Get all file paths in the folder\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path)]\n",
    "# Open file streams\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = await client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "# Close file streams\n",
    "for file in file_streams:\n",
    "    file.close()\n",
    "\n",
    "#### Create an assistant with file search ####\n",
    "product_agent = OpenAIAssistantAgent(\n",
    "    name=\"ProductAgent\",\n",
    "    description=\"Suggestions for products\",\n",
    "    client=client,\n",
    "    model=azure_openai_deployment,\n",
    "    instructions=\"You will provide possible products for the hiker based on the file search tool such as the TrailMaster X4 Tent, CozyNights Sleeping Bag, and more.\",\n",
    "    tools=[\"file_search\"],\n",
    "    tool_resources={\"file_search\":{\"vector_store_ids\":[vector_store.id]}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the product agent\n",
    "user_input = TextMessage(source=\"user\", content=\"What is the price of the TrailMaster X4 Tent?\")\n",
    "response = await product_agent.on_messages([user_input], cancellation_token\n",
    ")\n",
    "print(response.chat_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the team\n",
    "Let’s create the team with two termination conditions: \n",
    "- TextMentionTermination to end the conversation when the User Proxy sends “APPROVE”\n",
    "- MaxMessageTermination to limit the conversation to avoid infinite loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mention_termination = TextMentionTermination(\"APPROVE\")\n",
    "max_messages_termination = MaxMessageTermination(max_messages=50)\n",
    "termination = text_mention_termination | max_messages_termination\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    [planning_agent, web_surfer_agent, hiking_agent, product_agent, writing_agent, critique_agent, user_proxy],\n",
    "    model_client=az_model_client,\n",
    "    termination_condition=termination,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Task and Run the Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"I want to hike in the Grand Canyon, Arizona, USA.\"\n",
    "# Use asyncio.run(...) if you are running this in a script.\n",
    "response = await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing whole response per agent source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape code for bold text\n",
    "bold_start = \"\\033[1m\"\n",
    "bold_end = \"\\033[0m\"\n",
    "\n",
    "# ANSI escape code for red text\n",
    "red_start = \"\\033[31m\"\n",
    "red_end = \"\\033[0m\"\n",
    "\n",
    "for messages in response.messages:\n",
    "    source = messages.source\n",
    "    print(f\"{bold_start}{red_start}{source}{bold_end}{red_end}\")\n",
    "    print(messages.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for messages in response.messages:\n",
    "    source = messages.source\n",
    "    print(f\"{bold_start}{red_start}{source}{bold_end}{red_end}\")\n",
    "    print(messages.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Selector Function\n",
    "Often times we want better control over the selection process. To this end, we can set the selector_func argument with a custom selector function to override the default model-based selection. For instance, we want the Planning Agent to speak immediately after any specialized agent to check the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function determines whether the last message in the sequence was sent by the planning_agent. \n",
    "# If it wasn't, the function returns the name of the planning_agent; otherwise, it returns None.\n",
    "# Returning None from the custom selector function will use the default model-based selection.\n",
    "def selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:\n",
    "    if messages[-1].source != planning_agent.name:\n",
    "        return planning_agent.name\n",
    "    return None\n",
    "\n",
    "\n",
    "# Reset the previous team and run the chat again with the selector function.\n",
    "await team.reset()\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    [planning_agent, web_surfer_agent, hiking_agent, product_agent, writing_agent, critique_agent, user_proxy],\n",
    "    model_client=az_model_client,\n",
    "    termination_condition=termination,\n",
    "    selector_func=selector_func,\n",
    ")\n",
    "\n",
    "response = await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for messages in response.messages:\n",
    "    source = messages.source\n",
    "    print(f\"{bold_start}{red_start}{source}{bold_end}{red_end}\")\n",
    "    print(messages.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the resources\n",
    "Clean up the OpenAI Assistant Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "await product_agent.delete_uploaded_files(cancellation_token)\n",
    "await product_agent.delete_assistant(cancellation_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
