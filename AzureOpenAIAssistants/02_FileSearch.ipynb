{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Assistants - File Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Create a client\n",
    "client = AzureOpenAI(\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_key\n",
    ")\n",
    "\n",
    "# Create a vector store\n",
    "vector_store = client.beta.vector_stores.create(name=\"Nasa Books\")\n",
    "\n",
    "# Specify the folder containing the files\n",
    "folder_path = \"../Data/nasabooks/\"\n",
    "\n",
    "# Get all file paths in the folder\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path)]\n",
    "\n",
    "# Open file streams\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "print(vector_store.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1-2:\n",
    "1. Create an Assistant\n",
    "2. Create a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Nasa books Assistant\",\n",
    "  instructions=\"\"\"\n",
    "  You are a assistant that provides information. \n",
    "   You will answer questions based on files provided to you about information in a NASA Book. \n",
    "   You will not provide answers outside of those files.\n",
    "  \"\"\",\n",
    "  model=azure_openai_deployment,\n",
    "  tools=[{\"type\":\"file_search\"}],\n",
    "  tool_resources={\"file_search\":{\"vector_store_ids\":[vector_store.id]}},\n",
    "  temperature=1,\n",
    "  top_p=1\n",
    ")\n",
    "\n",
    "# Step 2: Create thread\n",
    "thread = client.beta.threads.create()\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3-6: \n",
    "3. Add a message to the thread\n",
    "4. Run the Assistant\n",
    "5. Check the Run Status\n",
    "6. Display the Assistant's Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "user_question =\"\"\"What can I see in the United States?\"\"\"\n",
    "\n",
    "# Step 3: Add a message to the thread\n",
    "message = client.beta.threads.messages.create(\n",
    "  thread_id=thread.id,\n",
    "  role=\"user\",\n",
    "  content=user_question\n",
    ")\n",
    "\n",
    "# Step 4: Run the Assistant\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "# Step 5: Check the Run Status\n",
    "# Looping until the run completes or fails\n",
    "while run.status in ['queued', 'in_progress', 'cancelling']:\n",
    "  time.sleep(1)\n",
    "  run = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    "  )\n",
    "\n",
    "  if run.status == 'completed':\n",
    "    messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "  )\n",
    "  \n",
    "  elif run.status == 'requires_action':\n",
    "    pass\n",
    "  \n",
    "  else:\n",
    "    print(run.status)\n",
    "\n",
    "# Step 6: Display the Assistant's Response\n",
    "content_block = messages.data[0].content[0]\n",
    "value = content_block.text.value\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat citations with the proper filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_citations(value):\n",
    "    # Extract the message content and annotations\n",
    "    message_content = messages.data[0].content[0]\n",
    "    annotations = message_content.text.annotations\n",
    "    \n",
    "    # Original response\n",
    "    paragraph = value\n",
    "\n",
    "    # Dictionary to store key-value pairs of text and filename\n",
    "    text_filename_pairs = {}\n",
    "\n",
    "    # Iterate over the annotations and extract the relevant information\n",
    "    for annotation in annotations:\n",
    "        file_id = annotation.file_citation.file_id\n",
    "        text = annotation.text\n",
    "        cited_file = client.files.retrieve(file_id)\n",
    "        filename = cited_file.filename\n",
    "\n",
    "        if text not in text_filename_pairs:\n",
    "            text_filename_pairs[text] = []\n",
    "        text_filename_pairs[text].append(filename)\n",
    "\n",
    "    # Print the key-value pairs\n",
    "    #for text, filenames in text_filename_pairs.items():\n",
    "    #    print(f'{text}: {\", \".join(filenames)}')\n",
    "\n",
    "    # Replace the citation texts with their corresponding filenames prefixed with \" Source: \"\n",
    "    for text, filenames in text_filename_pairs.items():\n",
    "        sources = \" Source: \" + \", \".join(filenames)\n",
    "        paragraph = paragraph.replace(text, sources)\n",
    "\n",
    "    return paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reformat_citations(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.beta.assistants.delete(assistant.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
